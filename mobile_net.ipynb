{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled32.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMq5JZiJM04eQvNAnkt0/nU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaos44/food_recognition/blob/master/mobile_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr1EW_iFPFYc"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)#2. Get the file\n",
        "d10 = drive.CreateFile({'id':'1jqXr5d-UAhOUaPIb-g7p_8xEW9XxHjUr'}) # replace the id with id of file you want to access\n",
        "d20 = drive.CreateFile({'id':'1hbZ19igWGti65MXOeGOwuzbSzVAOLdlH'})\n",
        "d30 = drive.CreateFile({'id':'12uU6ocY9r6BfBk8fo4BQC8BVXjDDhCBb'})\n",
        "d40 = drive.CreateFile({'id':'1Hs5OVtYD8xDvLd7y6drDHUu4ErXjBvr_'})\n",
        "d50 = drive.CreateFile({'id':'19FaeD7fdFlqu0U8K2DIJevtp1tb4PbBe'})\n",
        "d10.GetContentFile('d10.zip')\n",
        "d20.GetContentFile('d20.zip')\n",
        "d30.GetContentFile('d30.zip')\n",
        "d40.GetContentFile('d40.zip')\n",
        "d50.GetContentFile('d50.zip')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VWopgt5PGpi"
      },
      "source": [
        "!unzip d10.zip -d food\n",
        "!unzip d20.zip -d food\n",
        "!unzip d30.zip -d food\n",
        "!unzip d40.zip -d food\n",
        "!unzip d50.zip -d food"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i5KL8EQRftJ"
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob\n",
        "import numpy as np\n",
        "from sklearn import model_selection\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkDCX1z4Rh03"
      },
      "source": [
        "# 中心から任意のサイズをトリミング\n",
        "def crop_center(pil_img, crop_width, crop_height):\n",
        "    img_width, img_height = pil_img.size\n",
        "    return pil_img.crop(((img_width - crop_width) // 2,\n",
        "                         (img_height - crop_height) // 2,\n",
        "                         (img_width + crop_width) // 2,\n",
        "                         (img_height + crop_height) // 2))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f57pwuZMRkkQ"
      },
      "source": [
        "# パラメータの初期化\n",
        "classes = ['beefdon', 'curry', 'fish', 'humberger', 'katsudon', 'ramen', 'seafooddon', 'soba', 'spaghetti', 'tempuradon'\n",
        "         , 'udon', 'lasagna', 'hamburger_steak', 'nikujaga', 'fried_rice', 'omelette', 'sukiyaki', 'sushi', 'gyoza', 'fried_chicken'\n",
        "         , 'okonomiyaki', 'croquette', 'pancake', 'french_toast', 'tart', 'taco_rice', 'doria', 'takoyaki', 'tapioca', 'yakisoba'\n",
        "         , 'ginger_pork', 'steak', 'roast_meat', 'bread', 'yakitori', 'rice_ball', 'goya_chanpuru', 'instant_noodle', 'meat_bun', 'gratin'\n",
        "         , 'mabo_tofu', 'tiramisu', 'cream_puff', 'eclair', 'hot_dog', 'potato_salad', 'stew', 'pizza', 'french_fry', 'avocado_salad']\n",
        "num_classes = len(classes)\n",
        "image_size = 150"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eja9JbwfR3Y7"
      },
      "source": [
        "# 画像の読み込みとnumpy配列への変換\n",
        "X_train = [] # リスト\n",
        "X_test = [] # リスト\n",
        "y_train = [] # リスト\n",
        "y_test = [] # リスト"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc4BXAubR5az"
      },
      "source": [
        "# アスペクト比を固定して、幅が指定した値になるようリサイズする。\n",
        "def scale_to_width(img, width):\n",
        "    height = round(img.height * width / img.width)\n",
        "    return img.resize((width, height))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOkTvUyRR65g",
        "outputId": "f7bf3fb6-9c7c-4c02-ca0a-bd317b93ca78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "for index, classlabel in enumerate(classes):\n",
        "    photos_dir = './food/' + classlabel  \n",
        "    files = glob.glob(photos_dir + '/*.jpg')\n",
        "    count = len(files)\n",
        "    # print(classlabel, count)  \n",
        "    for i, file in enumerate(files):\n",
        "        image = Image.open(file)\n",
        "        image = image.convert('RGB')\n",
        "        # image = image.convert('RGB')の前にトリミングするとメモリオーバー、理由不明\n",
        "        # 食材ごとにテストとトレーニングに分ける\n",
        "        # 画像サイズの取得\n",
        "        # 長い方に沿ってトリミング\n",
        "        # image = crop_center(image, 400, 400) # resizeする時にアスペクト比を固定させるため、400*400の部分をトリミング(800 * 800より正解率が上がる)\n",
        "        w, h = image.size\n",
        "        image = crop_center(image, w, w) if w >= h else crop_center(image, h, h)\n",
        "        image = image.resize((image_size, image_size))\n",
        "        data = np.asarray(image, dtype=np.float32) / 255.0  # 正規化 # dtype=np.float32でサイズ縮小、MemoryError解消のため # 2020/7/7\n",
        "        if (i < 3/4 * count):\n",
        "            X_train.append(data)\n",
        "            y_train.append(index)\n",
        "        else: \n",
        "            X_test.append(data)\n",
        "            y_test.append(index)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 2. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAQHkRZzR9J6",
        "outputId": "7510da3e-e54e-41c6-c6b9-8e5bece94425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(X_train.nbytes)\n",
        "print(X_test.nbytes)\n",
        "print(y_train.nbytes)\n",
        "print(y_test.nbytes)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3431160000\n",
            "1136700000\n",
            "101664\n",
            "33680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxW2Jv5OR_W_"
      },
      "source": [
        "# X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y)\n",
        "# X_train, X_test = model_selection.train_test_split(X)\n",
        "# print(X_train.nbytes)\n",
        "# print(X_test.nbytes)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "monlBiTnSGjw"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.utils import np_utils\n",
        "from keras.applications import MobileNetV2\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjNdn7HjsbtN"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am9hgLxfsfpY"
      },
      "source": [
        "# compute quantities requiredXXfor featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied)\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xck7RMIf1BMz",
        "outputId": "329fd437-fbd0-4501-b919-85a34a6b5df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# mobile_net\n",
        "model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
        "top_model = Sequential()\n",
        "top_model.add(Flatten(input_shape=model.output_shape[1:])) # 1番目は個数、その後の数を使う\n",
        "top_model.add(Dense(256, activation='relu'))\n",
        "top_model.add(Dropout(0.5))\n",
        "top_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model = Model(inputs=model.input, outputs=top_model(model.output))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWgCsvWpPl3k",
        "outputId": "967c2443-7e19-4257-b4ee-7fee8d9bfb21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# レイアを調べる\n",
        "for i, layer in enumerate(model.layers):\n",
        "    print(i, layer.name)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 input_1\n",
            "1 Conv1_pad\n",
            "2 Conv1\n",
            "3 bn_Conv1\n",
            "4 Conv1_relu\n",
            "5 expanded_conv_depthwise\n",
            "6 expanded_conv_depthwise_BN\n",
            "7 expanded_conv_depthwise_relu\n",
            "8 expanded_conv_project\n",
            "9 expanded_conv_project_BN\n",
            "10 block_1_expand\n",
            "11 block_1_expand_BN\n",
            "12 block_1_expand_relu\n",
            "13 block_1_pad\n",
            "14 block_1_depthwise\n",
            "15 block_1_depthwise_BN\n",
            "16 block_1_depthwise_relu\n",
            "17 block_1_project\n",
            "18 block_1_project_BN\n",
            "19 block_2_expand\n",
            "20 block_2_expand_BN\n",
            "21 block_2_expand_relu\n",
            "22 block_2_depthwise\n",
            "23 block_2_depthwise_BN\n",
            "24 block_2_depthwise_relu\n",
            "25 block_2_project\n",
            "26 block_2_project_BN\n",
            "27 block_2_add\n",
            "28 block_3_expand\n",
            "29 block_3_expand_BN\n",
            "30 block_3_expand_relu\n",
            "31 block_3_pad\n",
            "32 block_3_depthwise\n",
            "33 block_3_depthwise_BN\n",
            "34 block_3_depthwise_relu\n",
            "35 block_3_project\n",
            "36 block_3_project_BN\n",
            "37 block_4_expand\n",
            "38 block_4_expand_BN\n",
            "39 block_4_expand_relu\n",
            "40 block_4_depthwise\n",
            "41 block_4_depthwise_BN\n",
            "42 block_4_depthwise_relu\n",
            "43 block_4_project\n",
            "44 block_4_project_BN\n",
            "45 block_4_add\n",
            "46 block_5_expand\n",
            "47 block_5_expand_BN\n",
            "48 block_5_expand_relu\n",
            "49 block_5_depthwise\n",
            "50 block_5_depthwise_BN\n",
            "51 block_5_depthwise_relu\n",
            "52 block_5_project\n",
            "53 block_5_project_BN\n",
            "54 block_5_add\n",
            "55 block_6_expand\n",
            "56 block_6_expand_BN\n",
            "57 block_6_expand_relu\n",
            "58 block_6_pad\n",
            "59 block_6_depthwise\n",
            "60 block_6_depthwise_BN\n",
            "61 block_6_depthwise_relu\n",
            "62 block_6_project\n",
            "63 block_6_project_BN\n",
            "64 block_7_expand\n",
            "65 block_7_expand_BN\n",
            "66 block_7_expand_relu\n",
            "67 block_7_depthwise\n",
            "68 block_7_depthwise_BN\n",
            "69 block_7_depthwise_relu\n",
            "70 block_7_project\n",
            "71 block_7_project_BN\n",
            "72 block_7_add\n",
            "73 block_8_expand\n",
            "74 block_8_expand_BN\n",
            "75 block_8_expand_relu\n",
            "76 block_8_depthwise\n",
            "77 block_8_depthwise_BN\n",
            "78 block_8_depthwise_relu\n",
            "79 block_8_project\n",
            "80 block_8_project_BN\n",
            "81 block_8_add\n",
            "82 block_9_expand\n",
            "83 block_9_expand_BN\n",
            "84 block_9_expand_relu\n",
            "85 block_9_depthwise\n",
            "86 block_9_depthwise_BN\n",
            "87 block_9_depthwise_relu\n",
            "88 block_9_project\n",
            "89 block_9_project_BN\n",
            "90 block_9_add\n",
            "91 block_10_expand\n",
            "92 block_10_expand_BN\n",
            "93 block_10_expand_relu\n",
            "94 block_10_depthwise\n",
            "95 block_10_depthwise_BN\n",
            "96 block_10_depthwise_relu\n",
            "97 block_10_project\n",
            "98 block_10_project_BN\n",
            "99 block_11_expand\n",
            "100 block_11_expand_BN\n",
            "101 block_11_expand_relu\n",
            "102 block_11_depthwise\n",
            "103 block_11_depthwise_BN\n",
            "104 block_11_depthwise_relu\n",
            "105 block_11_project\n",
            "106 block_11_project_BN\n",
            "107 block_11_add\n",
            "108 block_12_expand\n",
            "109 block_12_expand_BN\n",
            "110 block_12_expand_relu\n",
            "111 block_12_depthwise\n",
            "112 block_12_depthwise_BN\n",
            "113 block_12_depthwise_relu\n",
            "114 block_12_project\n",
            "115 block_12_project_BN\n",
            "116 block_12_add\n",
            "117 block_13_expand\n",
            "118 block_13_expand_BN\n",
            "119 block_13_expand_relu\n",
            "120 block_13_pad\n",
            "121 block_13_depthwise\n",
            "122 block_13_depthwise_BN\n",
            "123 block_13_depthwise_relu\n",
            "124 block_13_project\n",
            "125 block_13_project_BN\n",
            "126 block_14_expand\n",
            "127 block_14_expand_BN\n",
            "128 block_14_expand_relu\n",
            "129 block_14_depthwise\n",
            "130 block_14_depthwise_BN\n",
            "131 block_14_depthwise_relu\n",
            "132 block_14_project\n",
            "133 block_14_project_BN\n",
            "134 block_14_add\n",
            "135 block_15_expand\n",
            "136 block_15_expand_BN\n",
            "137 block_15_expand_relu\n",
            "138 block_15_depthwise\n",
            "139 block_15_depthwise_BN\n",
            "140 block_15_depthwise_relu\n",
            "141 block_15_project\n",
            "142 block_15_project_BN\n",
            "143 block_15_add\n",
            "144 block_16_expand\n",
            "145 block_16_expand_BN\n",
            "146 block_16_expand_relu\n",
            "147 block_16_depthwise\n",
            "148 block_16_depthwise_BN\n",
            "149 block_16_depthwise_relu\n",
            "150 block_16_project\n",
            "151 block_16_project_BN\n",
            "152 Conv_1\n",
            "153 Conv_1_bn\n",
            "154 out_relu\n",
            "155 sequential\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLA4PtdyPhCt",
        "outputId": "61914b9b-4ad3-44b8-8ab6-5f7c4e99f6e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 最後のブロックだけ学習させる\n",
        "for layer in model.layers[0:154]:\n",
        "    layer.trainable = False\n",
        "\n",
        "opt = Adam(lr=0.0001)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "                    steps_per_epoch=len(X_train) / 32, epochs=200)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, batch_size=32)\n",
        "print(score)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 3.4411 - accuracy: 0.1496\n",
            "Epoch 2/200\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 2.8390 - accuracy: 0.2832\n",
            "Epoch 3/200\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 2.4915 - accuracy: 0.3591\n",
            "Epoch 4/200\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 2.3289 - accuracy: 0.4005\n",
            "Epoch 5/200\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 2.2274 - accuracy: 0.4227\n",
            "Epoch 6/200\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 2.1049 - accuracy: 0.4448\n",
            "Epoch 7/200\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 2.0131 - accuracy: 0.4608\n",
            "Epoch 8/200\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 1.9619 - accuracy: 0.4785\n",
            "Epoch 9/200\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 1.9178 - accuracy: 0.4888\n",
            "Epoch 10/200\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 1.8792 - accuracy: 0.4985\n",
            "Epoch 11/200\n",
            "398/397 [==============================] - 63s 157ms/step - loss: 1.8208 - accuracy: 0.5075\n",
            "Epoch 12/200\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 1.8077 - accuracy: 0.5155\n",
            "Epoch 13/200\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 1.7592 - accuracy: 0.5208\n",
            "Epoch 14/200\n",
            "398/397 [==============================] - 62s 157ms/step - loss: 1.7239 - accuracy: 0.5281\n",
            "Epoch 15/200\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 1.7046 - accuracy: 0.5371\n",
            "Epoch 16/200\n",
            "398/397 [==============================] - 62s 156ms/step - loss: 1.6792 - accuracy: 0.5404\n",
            "Epoch 17/200\n",
            "398/397 [==============================] - 62s 156ms/step - loss: 1.6489 - accuracy: 0.5462\n",
            "Epoch 18/200\n",
            "398/397 [==============================] - 62s 156ms/step - loss: 1.6212 - accuracy: 0.5530\n",
            "Epoch 19/200\n",
            "398/397 [==============================] - 62s 155ms/step - loss: 1.5976 - accuracy: 0.5630\n",
            "Epoch 20/200\n",
            "398/397 [==============================] - 62s 156ms/step - loss: 1.5688 - accuracy: 0.5667\n",
            "Epoch 21/200\n",
            "398/397 [==============================] - 63s 157ms/step - loss: 1.5811 - accuracy: 0.5589\n",
            "Epoch 22/200\n",
            "398/397 [==============================] - 63s 157ms/step - loss: 1.5488 - accuracy: 0.5702\n",
            "Epoch 23/200\n",
            "398/397 [==============================] - 63s 157ms/step - loss: 1.5277 - accuracy: 0.5760\n",
            "Epoch 24/200\n",
            "398/397 [==============================] - 62s 156ms/step - loss: 1.4950 - accuracy: 0.5809\n",
            "Epoch 25/200\n",
            "398/397 [==============================] - 64s 161ms/step - loss: 1.4689 - accuracy: 0.5906\n",
            "Epoch 26/200\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 1.4737 - accuracy: 0.5895\n",
            "Epoch 27/200\n",
            "398/397 [==============================] - 65s 164ms/step - loss: 1.4467 - accuracy: 0.5936\n",
            "Epoch 28/200\n",
            "398/397 [==============================] - 64s 162ms/step - loss: 1.4388 - accuracy: 0.5932\n",
            "Epoch 29/200\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 1.4345 - accuracy: 0.5937\n",
            "Epoch 30/200\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 1.4185 - accuracy: 0.6005\n",
            "Epoch 31/200\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 1.3999 - accuracy: 0.6048\n",
            "Epoch 32/200\n",
            "398/397 [==============================] - 67s 167ms/step - loss: 1.4020 - accuracy: 0.6017\n",
            "Epoch 33/200\n",
            "398/397 [==============================] - 66s 167ms/step - loss: 1.3815 - accuracy: 0.6132\n",
            "Epoch 34/200\n",
            "398/397 [==============================] - 67s 167ms/step - loss: 1.3663 - accuracy: 0.6127\n",
            "Epoch 35/200\n",
            "398/397 [==============================] - 66s 167ms/step - loss: 1.3799 - accuracy: 0.6065\n",
            "Epoch 36/200\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 1.3683 - accuracy: 0.6116\n",
            "Epoch 37/200\n",
            "398/397 [==============================] - 66s 165ms/step - loss: 1.3318 - accuracy: 0.6213\n",
            "Epoch 38/200\n",
            "398/397 [==============================] - 66s 165ms/step - loss: 1.3536 - accuracy: 0.6195\n",
            "Epoch 39/200\n",
            "398/397 [==============================] - 66s 167ms/step - loss: 1.3493 - accuracy: 0.6179\n",
            "Epoch 40/200\n",
            "398/397 [==============================] - 67s 167ms/step - loss: 1.3511 - accuracy: 0.6205\n",
            "Epoch 41/200\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 1.3091 - accuracy: 0.6232\n",
            "Epoch 42/200\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 1.3135 - accuracy: 0.6252\n",
            "Epoch 43/200\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 1.3020 - accuracy: 0.6289\n",
            "Epoch 44/200\n",
            "398/397 [==============================] - 67s 167ms/step - loss: 1.2757 - accuracy: 0.6302\n",
            "Epoch 45/200\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 1.2942 - accuracy: 0.6244\n",
            "Epoch 46/200\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 1.2855 - accuracy: 0.6341\n",
            "Epoch 47/200\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 1.2596 - accuracy: 0.6409\n",
            "Epoch 48/200\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 1.2670 - accuracy: 0.6351\n",
            "Epoch 49/200\n",
            "398/397 [==============================] - 68s 172ms/step - loss: 1.2601 - accuracy: 0.6369\n",
            "Epoch 50/200\n",
            "398/397 [==============================] - 67s 170ms/step - loss: 1.2741 - accuracy: 0.6348\n",
            "Epoch 51/200\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 1.2603 - accuracy: 0.6374\n",
            "Epoch 52/200\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 1.2418 - accuracy: 0.6404\n",
            "Epoch 53/200\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 1.2256 - accuracy: 0.6454\n",
            "Epoch 54/200\n",
            "398/397 [==============================] - 69s 172ms/step - loss: 1.2319 - accuracy: 0.6454\n",
            "Epoch 55/200\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 1.2128 - accuracy: 0.6515\n",
            "Epoch 56/200\n",
            "398/397 [==============================] - 66s 167ms/step - loss: 1.2113 - accuracy: 0.6531\n",
            "Epoch 57/200\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 1.2111 - accuracy: 0.6510\n",
            "Epoch 58/200\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 1.1880 - accuracy: 0.6582\n",
            "Epoch 59/200\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 1.2001 - accuracy: 0.6515\n",
            "Epoch 60/200\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 1.1852 - accuracy: 0.6582\n",
            "Epoch 61/200\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 1.1956 - accuracy: 0.6569\n",
            "Epoch 62/200\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 1.1770 - accuracy: 0.6598\n",
            "Epoch 63/200\n",
            "398/397 [==============================] - 67s 167ms/step - loss: 1.1597 - accuracy: 0.6627\n",
            "Epoch 64/200\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 1.1571 - accuracy: 0.6617\n",
            "Epoch 65/200\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 1.1572 - accuracy: 0.6612\n",
            "Epoch 66/200\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 1.1549 - accuracy: 0.6648\n",
            "Epoch 67/200\n",
            "398/397 [==============================] - 65s 164ms/step - loss: 1.1422 - accuracy: 0.6656\n",
            "Epoch 68/200\n",
            "398/397 [==============================] - 65s 164ms/step - loss: 1.1415 - accuracy: 0.6684\n",
            "Epoch 69/200\n",
            "398/397 [==============================] - 66s 167ms/step - loss: 1.1433 - accuracy: 0.6690\n",
            "Epoch 70/200\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 1.1252 - accuracy: 0.6689\n",
            "Epoch 71/200\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 1.1327 - accuracy: 0.6703\n",
            "Epoch 72/200\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 1.1366 - accuracy: 0.6701\n",
            "Epoch 73/200\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 1.1403 - accuracy: 0.6656\n",
            "Epoch 74/200\n",
            "398/397 [==============================] - 67s 167ms/step - loss: 1.1088 - accuracy: 0.6773\n",
            "Epoch 75/200\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 1.0928 - accuracy: 0.6840\n",
            "Epoch 76/200\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 1.0997 - accuracy: 0.6760\n",
            "Epoch 77/200\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 1.1003 - accuracy: 0.6761\n",
            "Epoch 78/200\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 1.1225 - accuracy: 0.6758\n",
            "Epoch 79/200\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 1.1253 - accuracy: 0.6711\n",
            "Epoch 80/200\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 1.0680 - accuracy: 0.6846\n",
            "Epoch 81/200\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 1.0949 - accuracy: 0.6808\n",
            "Epoch 82/200\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 1.0805 - accuracy: 0.6837\n",
            "Epoch 83/200\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 1.0729 - accuracy: 0.6824\n",
            "Epoch 84/200\n",
            "398/397 [==============================] - 67s 167ms/step - loss: 1.0760 - accuracy: 0.6845\n",
            "Epoch 85/200\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 1.0756 - accuracy: 0.6871\n",
            "Epoch 86/200\n",
            "398/397 [==============================] - 66s 167ms/step - loss: 1.0685 - accuracy: 0.6838\n",
            "Epoch 87/200\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 1.0727 - accuracy: 0.6867\n",
            "Epoch 88/200\n",
            "398/397 [==============================] - 65s 164ms/step - loss: 1.0599 - accuracy: 0.6874\n",
            "Epoch 89/200\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 1.0696 - accuracy: 0.6868\n",
            "Epoch 90/200\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 1.0494 - accuracy: 0.6898\n",
            "Epoch 91/200\n",
            "398/397 [==============================] - 64s 161ms/step - loss: 1.0495 - accuracy: 0.6888\n",
            "Epoch 92/200\n",
            "398/397 [==============================] - 64s 162ms/step - loss: 1.0496 - accuracy: 0.6924\n",
            "Epoch 93/200\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 1.0587 - accuracy: 0.6885\n",
            "Epoch 94/200\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 1.0586 - accuracy: 0.6906\n",
            "Epoch 95/200\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 1.0290 - accuracy: 0.6970\n",
            "Epoch 96/200\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 1.0489 - accuracy: 0.6865\n",
            "Epoch 97/200\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 1.0407 - accuracy: 0.6934\n",
            "Epoch 98/200\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 1.0408 - accuracy: 0.6934\n",
            "Epoch 99/200\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 1.0150 - accuracy: 0.6983\n",
            "Epoch 100/200\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 1.0358 - accuracy: 0.6981\n",
            "Epoch 101/200\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 1.0314 - accuracy: 0.6945\n",
            "Epoch 102/200\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 1.0364 - accuracy: 0.6908\n",
            "Epoch 103/200\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 1.0127 - accuracy: 0.6975\n",
            "Epoch 104/200\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 1.0174 - accuracy: 0.6986\n",
            "Epoch 105/200\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 1.0245 - accuracy: 0.6952\n",
            "Epoch 106/200\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 1.0232 - accuracy: 0.6969\n",
            "Epoch 107/200\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 1.0048 - accuracy: 0.6989\n",
            "Epoch 108/200\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 0.9963 - accuracy: 0.7064\n",
            "Epoch 109/200\n",
            "398/397 [==============================] - 64s 161ms/step - loss: 1.0092 - accuracy: 0.7044\n",
            "Epoch 110/200\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 1.0090 - accuracy: 0.6965\n",
            "Epoch 111/200\n",
            "398/397 [==============================] - 63s 157ms/step - loss: 0.9924 - accuracy: 0.7011\n",
            "Epoch 112/200\n",
            "398/397 [==============================] - 62s 157ms/step - loss: 1.0031 - accuracy: 0.7045\n",
            "Epoch 113/200\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 1.0029 - accuracy: 0.7040\n",
            "Epoch 114/200\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 0.9869 - accuracy: 0.7102\n",
            "Epoch 115/200\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 1.0106 - accuracy: 0.7002\n",
            "Epoch 116/200\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 0.9902 - accuracy: 0.7112\n",
            "Epoch 117/200\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 0.9635 - accuracy: 0.7144\n",
            "Epoch 118/200\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 0.9733 - accuracy: 0.7089\n",
            "Epoch 119/200\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 0.9809 - accuracy: 0.7122\n",
            "Epoch 120/200\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 0.9801 - accuracy: 0.7127\n",
            "Epoch 121/200\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 0.9738 - accuracy: 0.7111\n",
            "Epoch 122/200\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 0.9645 - accuracy: 0.7100\n",
            "Epoch 123/200\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 0.9569 - accuracy: 0.7142\n",
            "Epoch 124/200\n",
            "398/397 [==============================] - 64s 162ms/step - loss: 0.9674 - accuracy: 0.7133\n",
            "Epoch 125/200\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 0.9685 - accuracy: 0.7118\n",
            "Epoch 126/200\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 0.9840 - accuracy: 0.7125\n",
            "Epoch 127/200\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 0.9634 - accuracy: 0.7104\n",
            "Epoch 128/200\n",
            "398/397 [==============================] - 65s 164ms/step - loss: 0.9416 - accuracy: 0.7163\n",
            "Epoch 129/200\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 0.9302 - accuracy: 0.7204\n",
            "Epoch 130/200\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 0.9390 - accuracy: 0.7161\n",
            "Epoch 131/200\n",
            "398/397 [==============================] - 66s 165ms/step - loss: 0.9724 - accuracy: 0.7104\n",
            "Epoch 132/200\n",
            "398/397 [==============================] - 66s 165ms/step - loss: 0.9711 - accuracy: 0.7074\n",
            "Epoch 133/200\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 0.9488 - accuracy: 0.7194\n",
            "Epoch 134/200\n",
            "398/397 [==============================] - 65s 164ms/step - loss: 0.9230 - accuracy: 0.7229\n",
            "Epoch 135/200\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 0.9193 - accuracy: 0.7198\n",
            "Epoch 136/200\n",
            "398/397 [==============================] - 65s 164ms/step - loss: 0.9326 - accuracy: 0.7196\n",
            "Epoch 137/200\n",
            "398/397 [==============================] - 65s 164ms/step - loss: 0.9287 - accuracy: 0.7195\n",
            "Epoch 138/200\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 0.9166 - accuracy: 0.7260\n",
            "Epoch 139/200\n",
            "398/397 [==============================] - 64s 162ms/step - loss: 0.9296 - accuracy: 0.7205\n",
            "Epoch 140/200\n",
            "398/397 [==============================] - 64s 162ms/step - loss: 0.9273 - accuracy: 0.7186\n",
            "Epoch 141/200\n",
            "398/397 [==============================] - 65s 162ms/step - loss: 0.9360 - accuracy: 0.7200\n",
            "Epoch 142/200\n",
            "398/397 [==============================] - 65s 162ms/step - loss: 0.9241 - accuracy: 0.7225\n",
            "Epoch 143/200\n",
            "398/397 [==============================] - 65s 162ms/step - loss: 0.9229 - accuracy: 0.7284\n",
            "Epoch 144/200\n",
            "398/397 [==============================] - 64s 161ms/step - loss: 0.9141 - accuracy: 0.7294\n",
            "Epoch 145/200\n",
            "398/397 [==============================] - 64s 161ms/step - loss: 0.9360 - accuracy: 0.7239\n",
            "Epoch 146/200\n",
            "398/397 [==============================] - 64s 162ms/step - loss: 0.9220 - accuracy: 0.7202\n",
            "Epoch 147/200\n",
            "388/397 [============================>.] - ETA: 1s - loss: 0.9167 - accuracy: 0.7247Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVtcBGmwEbh6",
        "outputId": "8b3eb8df-6ed7-4088-c333-fa9237a72407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "score = model.evaluate(X_test, y_test, batch_size=32)\n",
        "print(score)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "132/132 [==============================] - 4s 34ms/step - loss: 1.3179 - accuracy: 0.7131\n",
            "[1.3178998231887817, 0.7130641341209412]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_ca7oh6gduB"
      },
      "source": [
        "model.save('./food_recognition_mobile.h5')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvT54if1gmTo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def process(data):\n",
        "    plt.figure()\n",
        "    plt.imshow(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2zr9jQORdML",
        "outputId": "cdb0cd9d-d607-4a05-8302-01c11428b35f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4210, 150, 150, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7l7T7oP3N5a"
      },
      "source": [
        "# 比較用 800*800 トリミング\n",
        "for index, classlabel in enumerate(classes):\n",
        "    photos_dir = './food/' + classlabel  \n",
        "    files = glob.glob(photos_dir + '/*.jpg')\n",
        "    for i, file in enumerate(files):\n",
        "        image = Image.open(file)\n",
        "        image = image.convert('RGB')\n",
        "        # image = image.convert('RGB')の前にトリミングするとメモリオーバー、理由不明\n",
        "        image = crop_center(image, 800, 800) # resizeする時にアスペクト比を固定させるため、800*800の部分をトリミング\n",
        "        image = image.resize((image_size, image_size))\n",
        "        data = np.asarray(image, dtype=np.float32) / 255.0  # 正規化 # dtype=np.float32でサイズ縮小、MemoryError解消のため # 2020/7/7\n",
        "        data = np.array(data)\n",
        "        process(data)\n",
        "        X = []\n",
        "        # 最後尾に追加する\n",
        "        X.append(data)\n",
        "        X = np.array(X)\n",
        "        result = model.predict([X])[0]\n",
        "        # 値の大きい方の番号(配列の添字)を返す\n",
        "        predicted = result.argmax()\n",
        "        percentage = int(result[predicted] * 100)\n",
        "        print(classes[predicted], percentage)\n",
        "        print()\n",
        "        if i > 0:\n",
        "          break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcEQS1dd58Wf"
      },
      "source": [
        "for index, classlabel in enumerate(classes):\n",
        "    photos_dir = './food/' + classlabel  \n",
        "    files = glob.glob(photos_dir + '/*.jpg')\n",
        "    for i, file in enumerate(files):\n",
        "        image = Image.open(file)\n",
        "        image = image.convert('RGB')\n",
        "        w, h = image.size\n",
        "        image = crop_center(image, w, w) if w >= h else crop_center(image, h, h)\n",
        "        image = image.resize((image_size, image_size))\n",
        "        data = np.asarray(image, dtype=np.float32) / 255.0  # 正規化 # dtype=np.float32でサイズ縮小、MemoryError解消のため # 2020/7/7\n",
        "        data = np.array(data)\n",
        "        process(data)\n",
        "        X = []\n",
        "        # 最後尾に追加する\n",
        "        X.append(data)\n",
        "        X = np.array(X)\n",
        "        result = model.predict([X])[0]\n",
        "        # 値の大きい方の番号(配列の添字)を返す\n",
        "        predicted = result.argmax()\n",
        "        percentage = int(result[predicted] * 100)\n",
        "        print(classes[predicted], percentage)\n",
        "        print()\n",
        "        if i > 0:\n",
        "          break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brnVukT0pf5J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}