{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled32.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUHCkoWpENcLz0vopsICbQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaos44/food_recognition/blob/master/EfficientNetB1~2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr1EW_iFPFYc"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)#2. Get the file\n",
        "d10 = drive.CreateFile({'id':'1jqXr5d-UAhOUaPIb-g7p_8xEW9XxHjUr'}) # replace the id with id of file you want to access\n",
        "d20 = drive.CreateFile({'id':'1hbZ19igWGti65MXOeGOwuzbSzVAOLdlH'})\n",
        "d30 = drive.CreateFile({'id':'12uU6ocY9r6BfBk8fo4BQC8BVXjDDhCBb'})\n",
        "d40 = drive.CreateFile({'id':'1Hs5OVtYD8xDvLd7y6drDHUu4ErXjBvr_'})\n",
        "d50 = drive.CreateFile({'id':'19FaeD7fdFlqu0U8K2DIJevtp1tb4PbBe'})\n",
        "d10.GetContentFile('d10.zip')\n",
        "d20.GetContentFile('d20.zip')\n",
        "d30.GetContentFile('d30.zip')\n",
        "d40.GetContentFile('d40.zip')\n",
        "d50.GetContentFile('d50.zip')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VWopgt5PGpi"
      },
      "source": [
        "!unzip d10.zip -d food\n",
        "!unzip d20.zip -d food\n",
        "!unzip d30.zip -d food\n",
        "!unzip d40.zip -d food\n",
        "!unzip d50.zip -d food"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i5KL8EQRftJ"
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob\n",
        "import numpy as np\n",
        "from sklearn import model_selection\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkDCX1z4Rh03"
      },
      "source": [
        "# 中心から任意のサイズをトリミング\n",
        "def crop_center(pil_img, crop_width, crop_height):\n",
        "    img_width, img_height = pil_img.size\n",
        "    return pil_img.crop(((img_width - crop_width) // 2,\n",
        "                         (img_height - crop_height) // 2,\n",
        "                         (img_width + crop_width) // 2,\n",
        "                         (img_height + crop_height) // 2))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f57pwuZMRkkQ"
      },
      "source": [
        "# パラメータの初期化\n",
        "classes = ['beefdon', 'curry', 'fish', 'humberger', 'katsudon', 'ramen', 'seafooddon', 'soba', 'spaghetti', 'tempuradon'\n",
        "         , 'udon', 'lasagna', 'hamburger_steak', 'nikujaga', 'fried_rice', 'omelette', 'sukiyaki', 'sushi', 'gyoza', 'fried_chicken'\n",
        "         , 'okonomiyaki', 'croquette', 'pancake', 'french_toast', 'tart', 'taco_rice', 'doria', 'takoyaki', 'tapioca', 'yakisoba'\n",
        "         , 'ginger_pork', 'steak', 'roast_meat', 'bread', 'yakitori', 'rice_ball', 'goya_chanpuru', 'instant_noodle', 'meat_bun', 'gratin'\n",
        "         , 'mabo_tofu', 'tiramisu', 'cream_puff', 'eclair', 'hot_dog', 'potato_salad', 'stew', 'pizza', 'french_fry', 'avocado_salad']\n",
        "num_classes = len(classes)\n",
        "image_size = 150"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eja9JbwfR3Y7"
      },
      "source": [
        "# 画像の読み込みとnumpy配列への変換\n",
        "X_train = [] # リスト\n",
        "X_test = [] # リスト\n",
        "y_train = [] # リスト\n",
        "y_test = [] # リスト"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc4BXAubR5az"
      },
      "source": [
        "# アスペクト比を固定して、幅が指定した値になるようリサイズする。\n",
        "def scale_to_width(img, width):\n",
        "    height = round(img.height * width / img.width)\n",
        "    return img.resize((width, height))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOkTvUyRR65g",
        "outputId": "2e6ae091-a88d-426d-da0f-30007d61f265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "for index, classlabel in enumerate(classes):\n",
        "    photos_dir = './food/' + classlabel  \n",
        "    files = glob.glob(photos_dir + '/*.jpg')\n",
        "    count = len(files)\n",
        "    # print(classlabel, count)  \n",
        "    for i, file in enumerate(files):\n",
        "        image = Image.open(file)\n",
        "        image = image.convert('RGB')\n",
        "        # image = image.convert('RGB')の前にトリミングするとメモリオーバー、理由不明\n",
        "        # 食材ごとにテストとトレーニングに分ける\n",
        "        # 画像サイズの取得\n",
        "        # 長い方に沿ってトリミング\n",
        "        # image = crop_center(image, 400, 400) # resizeする時にアスペクト比を固定させるため、400*400の部分をトリミング(800 * 800より正解率が上がる)\n",
        "        w, h = image.size\n",
        "        image = crop_center(image, w, w) if w >= h else crop_center(image, h, h)\n",
        "        image = image.resize((image_size, image_size))\n",
        "        data = np.asarray(image, dtype=np.float32) / 255.0  # 正規化 # dtype=np.float32でサイズ縮小、MemoryError解消のため # 2020/7/7\n",
        "        if (i < 3/4 * count):\n",
        "            X_train.append(data)\n",
        "            y_train.append(index)\n",
        "        else: \n",
        "            X_test.append(data)\n",
        "            y_test.append(index)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 2. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAQHkRZzR9J6",
        "outputId": "3c770d18-6ced-4086-bc9d-098d65320244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(X_train.nbytes)\n",
        "print(X_test.nbytes)\n",
        "print(y_train.nbytes)\n",
        "print(y_test.nbytes)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3431160000\n",
            "1136700000\n",
            "101664\n",
            "33680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxW2Jv5OR_W_"
      },
      "source": [
        "# X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y)\n",
        "# X_train, X_test = model_selection.train_test_split(X)\n",
        "# print(X_train.nbytes)\n",
        "# print(X_test.nbytes)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "monlBiTnSGjw"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.applications import EfficientNetB2\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjNdn7HjsbtN"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am9hgLxfsfpY"
      },
      "source": [
        "# compute quantities requiredXXfor featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied)\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xck7RMIf1BMz"
      },
      "source": [
        "# mobile_net\n",
        "model = EfficientNetB2(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
        "top_model = Sequential()\n",
        "top_model.add(Flatten(input_shape=model.output_shape[1:])) # 1番目は個数、その後の数を使う\n",
        "top_model.add(Dense(256, activation='relu'))\n",
        "top_model.add(Dropout(0.5))\n",
        "top_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model = Model(inputs=model.input, outputs=top_model(model.output))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWgCsvWpPl3k",
        "outputId": "6d7c753a-6299-4b7a-a56f-606e5c1a667a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# レイアを調べる\n",
        "for i, layer in enumerate(model.layers):\n",
        "    print(i, layer.name)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 input_1\n",
            "1 rescaling\n",
            "2 normalization\n",
            "3 stem_conv_pad\n",
            "4 stem_conv\n",
            "5 stem_bn\n",
            "6 stem_activation\n",
            "7 block1a_dwconv\n",
            "8 block1a_bn\n",
            "9 block1a_activation\n",
            "10 block1a_se_squeeze\n",
            "11 block1a_se_reshape\n",
            "12 block1a_se_reduce\n",
            "13 block1a_se_expand\n",
            "14 block1a_se_excite\n",
            "15 block1a_project_conv\n",
            "16 block1a_project_bn\n",
            "17 block1b_dwconv\n",
            "18 block1b_bn\n",
            "19 block1b_activation\n",
            "20 block1b_se_squeeze\n",
            "21 block1b_se_reshape\n",
            "22 block1b_se_reduce\n",
            "23 block1b_se_expand\n",
            "24 block1b_se_excite\n",
            "25 block1b_project_conv\n",
            "26 block1b_project_bn\n",
            "27 block1b_drop\n",
            "28 block1b_add\n",
            "29 block2a_expand_conv\n",
            "30 block2a_expand_bn\n",
            "31 block2a_expand_activation\n",
            "32 block2a_dwconv_pad\n",
            "33 block2a_dwconv\n",
            "34 block2a_bn\n",
            "35 block2a_activation\n",
            "36 block2a_se_squeeze\n",
            "37 block2a_se_reshape\n",
            "38 block2a_se_reduce\n",
            "39 block2a_se_expand\n",
            "40 block2a_se_excite\n",
            "41 block2a_project_conv\n",
            "42 block2a_project_bn\n",
            "43 block2b_expand_conv\n",
            "44 block2b_expand_bn\n",
            "45 block2b_expand_activation\n",
            "46 block2b_dwconv\n",
            "47 block2b_bn\n",
            "48 block2b_activation\n",
            "49 block2b_se_squeeze\n",
            "50 block2b_se_reshape\n",
            "51 block2b_se_reduce\n",
            "52 block2b_se_expand\n",
            "53 block2b_se_excite\n",
            "54 block2b_project_conv\n",
            "55 block2b_project_bn\n",
            "56 block2b_drop\n",
            "57 block2b_add\n",
            "58 block2c_expand_conv\n",
            "59 block2c_expand_bn\n",
            "60 block2c_expand_activation\n",
            "61 block2c_dwconv\n",
            "62 block2c_bn\n",
            "63 block2c_activation\n",
            "64 block2c_se_squeeze\n",
            "65 block2c_se_reshape\n",
            "66 block2c_se_reduce\n",
            "67 block2c_se_expand\n",
            "68 block2c_se_excite\n",
            "69 block2c_project_conv\n",
            "70 block2c_project_bn\n",
            "71 block2c_drop\n",
            "72 block2c_add\n",
            "73 block3a_expand_conv\n",
            "74 block3a_expand_bn\n",
            "75 block3a_expand_activation\n",
            "76 block3a_dwconv_pad\n",
            "77 block3a_dwconv\n",
            "78 block3a_bn\n",
            "79 block3a_activation\n",
            "80 block3a_se_squeeze\n",
            "81 block3a_se_reshape\n",
            "82 block3a_se_reduce\n",
            "83 block3a_se_expand\n",
            "84 block3a_se_excite\n",
            "85 block3a_project_conv\n",
            "86 block3a_project_bn\n",
            "87 block3b_expand_conv\n",
            "88 block3b_expand_bn\n",
            "89 block3b_expand_activation\n",
            "90 block3b_dwconv\n",
            "91 block3b_bn\n",
            "92 block3b_activation\n",
            "93 block3b_se_squeeze\n",
            "94 block3b_se_reshape\n",
            "95 block3b_se_reduce\n",
            "96 block3b_se_expand\n",
            "97 block3b_se_excite\n",
            "98 block3b_project_conv\n",
            "99 block3b_project_bn\n",
            "100 block3b_drop\n",
            "101 block3b_add\n",
            "102 block3c_expand_conv\n",
            "103 block3c_expand_bn\n",
            "104 block3c_expand_activation\n",
            "105 block3c_dwconv\n",
            "106 block3c_bn\n",
            "107 block3c_activation\n",
            "108 block3c_se_squeeze\n",
            "109 block3c_se_reshape\n",
            "110 block3c_se_reduce\n",
            "111 block3c_se_expand\n",
            "112 block3c_se_excite\n",
            "113 block3c_project_conv\n",
            "114 block3c_project_bn\n",
            "115 block3c_drop\n",
            "116 block3c_add\n",
            "117 block4a_expand_conv\n",
            "118 block4a_expand_bn\n",
            "119 block4a_expand_activation\n",
            "120 block4a_dwconv_pad\n",
            "121 block4a_dwconv\n",
            "122 block4a_bn\n",
            "123 block4a_activation\n",
            "124 block4a_se_squeeze\n",
            "125 block4a_se_reshape\n",
            "126 block4a_se_reduce\n",
            "127 block4a_se_expand\n",
            "128 block4a_se_excite\n",
            "129 block4a_project_conv\n",
            "130 block4a_project_bn\n",
            "131 block4b_expand_conv\n",
            "132 block4b_expand_bn\n",
            "133 block4b_expand_activation\n",
            "134 block4b_dwconv\n",
            "135 block4b_bn\n",
            "136 block4b_activation\n",
            "137 block4b_se_squeeze\n",
            "138 block4b_se_reshape\n",
            "139 block4b_se_reduce\n",
            "140 block4b_se_expand\n",
            "141 block4b_se_excite\n",
            "142 block4b_project_conv\n",
            "143 block4b_project_bn\n",
            "144 block4b_drop\n",
            "145 block4b_add\n",
            "146 block4c_expand_conv\n",
            "147 block4c_expand_bn\n",
            "148 block4c_expand_activation\n",
            "149 block4c_dwconv\n",
            "150 block4c_bn\n",
            "151 block4c_activation\n",
            "152 block4c_se_squeeze\n",
            "153 block4c_se_reshape\n",
            "154 block4c_se_reduce\n",
            "155 block4c_se_expand\n",
            "156 block4c_se_excite\n",
            "157 block4c_project_conv\n",
            "158 block4c_project_bn\n",
            "159 block4c_drop\n",
            "160 block4c_add\n",
            "161 block4d_expand_conv\n",
            "162 block4d_expand_bn\n",
            "163 block4d_expand_activation\n",
            "164 block4d_dwconv\n",
            "165 block4d_bn\n",
            "166 block4d_activation\n",
            "167 block4d_se_squeeze\n",
            "168 block4d_se_reshape\n",
            "169 block4d_se_reduce\n",
            "170 block4d_se_expand\n",
            "171 block4d_se_excite\n",
            "172 block4d_project_conv\n",
            "173 block4d_project_bn\n",
            "174 block4d_drop\n",
            "175 block4d_add\n",
            "176 block5a_expand_conv\n",
            "177 block5a_expand_bn\n",
            "178 block5a_expand_activation\n",
            "179 block5a_dwconv\n",
            "180 block5a_bn\n",
            "181 block5a_activation\n",
            "182 block5a_se_squeeze\n",
            "183 block5a_se_reshape\n",
            "184 block5a_se_reduce\n",
            "185 block5a_se_expand\n",
            "186 block5a_se_excite\n",
            "187 block5a_project_conv\n",
            "188 block5a_project_bn\n",
            "189 block5b_expand_conv\n",
            "190 block5b_expand_bn\n",
            "191 block5b_expand_activation\n",
            "192 block5b_dwconv\n",
            "193 block5b_bn\n",
            "194 block5b_activation\n",
            "195 block5b_se_squeeze\n",
            "196 block5b_se_reshape\n",
            "197 block5b_se_reduce\n",
            "198 block5b_se_expand\n",
            "199 block5b_se_excite\n",
            "200 block5b_project_conv\n",
            "201 block5b_project_bn\n",
            "202 block5b_drop\n",
            "203 block5b_add\n",
            "204 block5c_expand_conv\n",
            "205 block5c_expand_bn\n",
            "206 block5c_expand_activation\n",
            "207 block5c_dwconv\n",
            "208 block5c_bn\n",
            "209 block5c_activation\n",
            "210 block5c_se_squeeze\n",
            "211 block5c_se_reshape\n",
            "212 block5c_se_reduce\n",
            "213 block5c_se_expand\n",
            "214 block5c_se_excite\n",
            "215 block5c_project_conv\n",
            "216 block5c_project_bn\n",
            "217 block5c_drop\n",
            "218 block5c_add\n",
            "219 block5d_expand_conv\n",
            "220 block5d_expand_bn\n",
            "221 block5d_expand_activation\n",
            "222 block5d_dwconv\n",
            "223 block5d_bn\n",
            "224 block5d_activation\n",
            "225 block5d_se_squeeze\n",
            "226 block5d_se_reshape\n",
            "227 block5d_se_reduce\n",
            "228 block5d_se_expand\n",
            "229 block5d_se_excite\n",
            "230 block5d_project_conv\n",
            "231 block5d_project_bn\n",
            "232 block5d_drop\n",
            "233 block5d_add\n",
            "234 block6a_expand_conv\n",
            "235 block6a_expand_bn\n",
            "236 block6a_expand_activation\n",
            "237 block6a_dwconv_pad\n",
            "238 block6a_dwconv\n",
            "239 block6a_bn\n",
            "240 block6a_activation\n",
            "241 block6a_se_squeeze\n",
            "242 block6a_se_reshape\n",
            "243 block6a_se_reduce\n",
            "244 block6a_se_expand\n",
            "245 block6a_se_excite\n",
            "246 block6a_project_conv\n",
            "247 block6a_project_bn\n",
            "248 block6b_expand_conv\n",
            "249 block6b_expand_bn\n",
            "250 block6b_expand_activation\n",
            "251 block6b_dwconv\n",
            "252 block6b_bn\n",
            "253 block6b_activation\n",
            "254 block6b_se_squeeze\n",
            "255 block6b_se_reshape\n",
            "256 block6b_se_reduce\n",
            "257 block6b_se_expand\n",
            "258 block6b_se_excite\n",
            "259 block6b_project_conv\n",
            "260 block6b_project_bn\n",
            "261 block6b_drop\n",
            "262 block6b_add\n",
            "263 block6c_expand_conv\n",
            "264 block6c_expand_bn\n",
            "265 block6c_expand_activation\n",
            "266 block6c_dwconv\n",
            "267 block6c_bn\n",
            "268 block6c_activation\n",
            "269 block6c_se_squeeze\n",
            "270 block6c_se_reshape\n",
            "271 block6c_se_reduce\n",
            "272 block6c_se_expand\n",
            "273 block6c_se_excite\n",
            "274 block6c_project_conv\n",
            "275 block6c_project_bn\n",
            "276 block6c_drop\n",
            "277 block6c_add\n",
            "278 block6d_expand_conv\n",
            "279 block6d_expand_bn\n",
            "280 block6d_expand_activation\n",
            "281 block6d_dwconv\n",
            "282 block6d_bn\n",
            "283 block6d_activation\n",
            "284 block6d_se_squeeze\n",
            "285 block6d_se_reshape\n",
            "286 block6d_se_reduce\n",
            "287 block6d_se_expand\n",
            "288 block6d_se_excite\n",
            "289 block6d_project_conv\n",
            "290 block6d_project_bn\n",
            "291 block6d_drop\n",
            "292 block6d_add\n",
            "293 block6e_expand_conv\n",
            "294 block6e_expand_bn\n",
            "295 block6e_expand_activation\n",
            "296 block6e_dwconv\n",
            "297 block6e_bn\n",
            "298 block6e_activation\n",
            "299 block6e_se_squeeze\n",
            "300 block6e_se_reshape\n",
            "301 block6e_se_reduce\n",
            "302 block6e_se_expand\n",
            "303 block6e_se_excite\n",
            "304 block6e_project_conv\n",
            "305 block6e_project_bn\n",
            "306 block6e_drop\n",
            "307 block6e_add\n",
            "308 block7a_expand_conv\n",
            "309 block7a_expand_bn\n",
            "310 block7a_expand_activation\n",
            "311 block7a_dwconv\n",
            "312 block7a_bn\n",
            "313 block7a_activation\n",
            "314 block7a_se_squeeze\n",
            "315 block7a_se_reshape\n",
            "316 block7a_se_reduce\n",
            "317 block7a_se_expand\n",
            "318 block7a_se_excite\n",
            "319 block7a_project_conv\n",
            "320 block7a_project_bn\n",
            "321 block7b_expand_conv\n",
            "322 block7b_expand_bn\n",
            "323 block7b_expand_activation\n",
            "324 block7b_dwconv\n",
            "325 block7b_bn\n",
            "326 block7b_activation\n",
            "327 block7b_se_squeeze\n",
            "328 block7b_se_reshape\n",
            "329 block7b_se_reduce\n",
            "330 block7b_se_expand\n",
            "331 block7b_se_excite\n",
            "332 block7b_project_conv\n",
            "333 block7b_project_bn\n",
            "334 block7b_drop\n",
            "335 block7b_add\n",
            "336 top_conv\n",
            "337 top_bn\n",
            "338 top_activation\n",
            "339 sequential\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLA4PtdyPhCt",
        "outputId": "300034a6-19e6-4288-9384-2829661b5fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        }
      },
      "source": [
        "# 最後のブロックだけ学習させる\n",
        "# for layer in model.layers[0:154]:\n",
        "#    layer.trainable = False\n",
        "\n",
        "# block_16学習\n",
        "for layer in model.layers[0:338]:\n",
        "    layer.trainable = False\n",
        "opt = Adam(lr=0.0001)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "                    steps_per_epoch=len(X_train) / 32, epochs=20)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, batch_size=32)\n",
        "print(score)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "398/397 [==============================] - 73s 184ms/step - loss: 3.9329 - accuracy: 0.0441\n",
            "Epoch 2/20\n",
            "398/397 [==============================] - 73s 184ms/step - loss: 3.9088 - accuracy: 0.0446\n",
            "Epoch 3/20\n",
            "398/397 [==============================] - 73s 184ms/step - loss: 3.9068 - accuracy: 0.0445\n",
            "Epoch 4/20\n",
            "398/397 [==============================] - 73s 185ms/step - loss: 3.9051 - accuracy: 0.0446\n",
            "Epoch 5/20\n",
            "398/397 [==============================] - 74s 186ms/step - loss: 3.9029 - accuracy: 0.0446\n",
            "Epoch 6/20\n",
            "398/397 [==============================] - 74s 185ms/step - loss: 3.9011 - accuracy: 0.0446\n",
            "Epoch 7/20\n",
            "398/397 [==============================] - 74s 185ms/step - loss: 3.9002 - accuracy: 0.0446\n",
            "Epoch 8/20\n",
            "398/397 [==============================] - 74s 185ms/step - loss: 3.8978 - accuracy: 0.0446\n",
            "Epoch 9/20\n",
            "398/397 [==============================] - 74s 186ms/step - loss: 3.8962 - accuracy: 0.0446\n",
            "Epoch 10/20\n",
            "398/397 [==============================] - 74s 185ms/step - loss: 3.8947 - accuracy: 0.0446\n",
            "Epoch 11/20\n",
            "398/397 [==============================] - 74s 185ms/step - loss: 3.8933 - accuracy: 0.0446\n",
            "Epoch 12/20\n",
            "398/397 [==============================] - 74s 185ms/step - loss: 3.8919 - accuracy: 0.0446\n",
            "Epoch 13/20\n",
            "398/397 [==============================] - 74s 185ms/step - loss: 3.8906 - accuracy: 0.0446\n",
            "Epoch 14/20\n",
            "398/397 [==============================] - 74s 185ms/step - loss: 3.8894 - accuracy: 0.0446\n",
            "Epoch 15/20\n",
            "398/397 [==============================] - 74s 185ms/step - loss: 3.8882 - accuracy: 0.0446\n",
            "Epoch 16/20\n",
            "398/397 [==============================] - 74s 186ms/step - loss: 3.8871 - accuracy: 0.0446\n",
            "Epoch 17/20\n",
            "398/397 [==============================] - 74s 185ms/step - loss: 3.8860 - accuracy: 0.0446\n",
            "Epoch 18/20\n",
            "398/397 [==============================] - 74s 185ms/step - loss: 3.8850 - accuracy: 0.0446\n",
            "Epoch 19/20\n",
            "398/397 [==============================] - 73s 184ms/step - loss: 3.8845 - accuracy: 0.0445\n",
            "Epoch 20/20\n",
            "398/397 [==============================] - 72s 182ms/step - loss: 3.8831 - accuracy: 0.0446\n",
            "132/132 [==============================] - 6s 48ms/step - loss: 3.8824 - accuracy: 0.0447\n",
            "[3.882431745529175, 0.044655583798885345]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_ca7oh6gduB"
      },
      "source": [
        "model.save('./food_recognition_mobile.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvT54if1gmTo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def process(data):\n",
        "    plt.figure()\n",
        "    plt.imshow(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2zr9jQORdML",
        "outputId": "cdb0cd9d-d607-4a05-8302-01c11428b35f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4210, 150, 150, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7l7T7oP3N5a"
      },
      "source": [
        "# 比較用 800*800 トリミング\n",
        "for index, classlabel in enumerate(classes):\n",
        "    photos_dir = './food/' + classlabel  \n",
        "    files = glob.glob(photos_dir + '/*.jpg')\n",
        "    for i, file in enumerate(files):\n",
        "        image = Image.open(file)\n",
        "        image = image.convert('RGB')\n",
        "        # image = image.convert('RGB')の前にトリミングするとメモリオーバー、理由不明\n",
        "        image = crop_center(image, 800, 800) # resizeする時にアスペクト比を固定させるため、800*800の部分をトリミング\n",
        "        image = image.resize((image_size, image_size))\n",
        "        data = np.asarray(image, dtype=np.float32) / 255.0  # 正規化 # dtype=np.float32でサイズ縮小、MemoryError解消のため # 2020/7/7\n",
        "        data = np.array(data)\n",
        "        process(data)\n",
        "        X = []\n",
        "        # 最後尾に追加する\n",
        "        X.append(data)\n",
        "        X = np.array(X)\n",
        "        result = model.predict([X])[0]\n",
        "        # 値の大きい方の番号(配列の添字)を返す\n",
        "        predicted = result.argmax()\n",
        "        percentage = int(result[predicted] * 100)\n",
        "        print(classes[predicted], percentage)\n",
        "        print()\n",
        "        if i > 0:\n",
        "          break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcEQS1dd58Wf"
      },
      "source": [
        "for index, classlabel in enumerate(classes):\n",
        "    photos_dir = './food/' + classlabel  \n",
        "    files = glob.glob(photos_dir + '/*.jpg')\n",
        "    for i, file in enumerate(files):\n",
        "        image = Image.open(file)\n",
        "        image = image.convert('RGB')\n",
        "        w, h = image.size\n",
        "        image = crop_center(image, w, w) if w >= h else crop_center(image, h, h)\n",
        "        image = image.resize((image_size, image_size))\n",
        "        data = np.asarray(image, dtype=np.float32) / 255.0  # 正規化 # dtype=np.float32でサイズ縮小、MemoryError解消のため # 2020/7/7\n",
        "        data = np.array(data)\n",
        "        process(data)\n",
        "        X = []\n",
        "        # 最後尾に追加する\n",
        "        X.append(data)\n",
        "        X = np.array(X)\n",
        "        result = model.predict([X])[0]\n",
        "        # 値の大きい方の番号(配列の添字)を返す\n",
        "        predicted = result.argmax()\n",
        "        percentage = int(result[predicted] * 100)\n",
        "        print(classes[predicted], percentage)\n",
        "        print()\n",
        "        if i > 0:\n",
        "          break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brnVukT0pf5J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}