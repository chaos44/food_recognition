{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled32.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMP0o0021VKNDIydO1IRCSb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaos44/food_recognition/blob/master/mobile_net_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr1EW_iFPFYc"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)#2. Get the file\n",
        "d10 = drive.CreateFile({'id':'1jqXr5d-UAhOUaPIb-g7p_8xEW9XxHjUr'}) # replace the id with id of file you want to access\n",
        "d20 = drive.CreateFile({'id':'1hbZ19igWGti65MXOeGOwuzbSzVAOLdlH'})\n",
        "d30 = drive.CreateFile({'id':'12uU6ocY9r6BfBk8fo4BQC8BVXjDDhCBb'})\n",
        "d40 = drive.CreateFile({'id':'1Hs5OVtYD8xDvLd7y6drDHUu4ErXjBvr_'})\n",
        "d50 = drive.CreateFile({'id':'19FaeD7fdFlqu0U8K2DIJevtp1tb4PbBe'})\n",
        "d10.GetContentFile('d10.zip')\n",
        "d20.GetContentFile('d20.zip')\n",
        "d30.GetContentFile('d30.zip')\n",
        "d40.GetContentFile('d40.zip')\n",
        "d50.GetContentFile('d50.zip')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VWopgt5PGpi"
      },
      "source": [
        "!unzip d10.zip -d food\n",
        "!unzip d20.zip -d food\n",
        "!unzip d30.zip -d food\n",
        "!unzip d40.zip -d food\n",
        "!unzip d50.zip -d food"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i5KL8EQRftJ"
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob\n",
        "import numpy as np\n",
        "from sklearn import model_selection\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkDCX1z4Rh03"
      },
      "source": [
        "# 中心から任意のサイズをトリミング\n",
        "def crop_center(pil_img, crop_width, crop_height):\n",
        "    img_width, img_height = pil_img.size\n",
        "    return pil_img.crop(((img_width - crop_width) // 2,\n",
        "                         (img_height - crop_height) // 2,\n",
        "                         (img_width + crop_width) // 2,\n",
        "                         (img_height + crop_height) // 2))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f57pwuZMRkkQ"
      },
      "source": [
        "# パラメータの初期化\n",
        "classes = ['beefdon', 'curry', 'fish', 'humberger', 'katsudon', 'ramen', 'seafooddon', 'soba', 'spaghetti', 'tempuradon'\n",
        "         , 'udon', 'lasagna', 'hamburger_steak', 'nikujaga', 'fried_rice', 'omelette', 'sukiyaki', 'sushi', 'gyoza', 'fried_chicken'\n",
        "         , 'okonomiyaki', 'croquette', 'pancake', 'french_toast', 'tart', 'taco_rice', 'doria', 'takoyaki', 'tapioca', 'yakisoba'\n",
        "         , 'ginger_pork', 'steak', 'roast_meat', 'bread', 'yakitori', 'rice_ball', 'goya_chanpuru', 'instant_noodle', 'meat_bun', 'gratin'\n",
        "         , 'mabo_tofu', 'tiramisu', 'cream_puff', 'eclair', 'hot_dog', 'potato_salad', 'stew', 'pizza', 'french_fry', 'avocado_salad']\n",
        "num_classes = len(classes)\n",
        "image_size = 150"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eja9JbwfR3Y7"
      },
      "source": [
        "# 画像の読み込みとnumpy配列への変換\n",
        "X_train = [] # リスト\n",
        "X_test = [] # リスト\n",
        "y_train = [] # リスト\n",
        "y_test = [] # リスト"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc4BXAubR5az"
      },
      "source": [
        "# アスペクト比を固定して、幅が指定した値になるようリサイズする。\n",
        "def scale_to_width(img, width):\n",
        "    height = round(img.height * width / img.width)\n",
        "    return img.resize((width, height))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOkTvUyRR65g",
        "outputId": "4ff8ebe0-21ee-4832-93f4-43acfcb86293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "for index, classlabel in enumerate(classes):\n",
        "    photos_dir = './food/' + classlabel  \n",
        "    files = glob.glob(photos_dir + '/*.jpg')\n",
        "    count = len(files)\n",
        "    # print(classlabel, count)  \n",
        "    for i, file in enumerate(files):\n",
        "        image = Image.open(file)\n",
        "        image = image.convert('RGB')\n",
        "        # image = image.convert('RGB')の前にトリミングするとメモリオーバー、理由不明\n",
        "        # 食材ごとにテストとトレーニングに分ける\n",
        "        # 画像サイズの取得\n",
        "        # 長い方に沿ってトリミング\n",
        "        # image = crop_center(image, 400, 400) # resizeする時にアスペクト比を固定させるため、400*400の部分をトリミング(800 * 800より正解率が上がる)\n",
        "        w, h = image.size\n",
        "        image = crop_center(image, w, w) if w >= h else crop_center(image, h, h)\n",
        "        image = image.resize((image_size, image_size))\n",
        "        data = np.asarray(image, dtype=np.float32) / 255.0  # 正規化 # dtype=np.float32でサイズ縮小、MemoryError解消のため # 2020/7/7\n",
        "        if (i < 3/4 * count):\n",
        "            X_train.append(data)\n",
        "            y_train.append(index)\n",
        "        else: \n",
        "            X_test.append(data)\n",
        "            y_test.append(index)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 2. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAQHkRZzR9J6",
        "outputId": "68cccb3f-3285-4b95-b01e-bed2c1cd1607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(X_train.nbytes)\n",
        "print(X_test.nbytes)\n",
        "print(y_train.nbytes)\n",
        "print(y_test.nbytes)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3431160000\n",
            "1136700000\n",
            "101664\n",
            "33680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxW2Jv5OR_W_"
      },
      "source": [
        "# X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y)\n",
        "# X_train, X_test = model_selection.train_test_split(X)\n",
        "# print(X_train.nbytes)\n",
        "# print(X_test.nbytes)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "monlBiTnSGjw"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.applications import EfficientNetB4\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjNdn7HjsbtN"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am9hgLxfsfpY"
      },
      "source": [
        "# compute quantities requiredXXfor featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied)\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xck7RMIf1BMz",
        "outputId": "39f76d51-66a6-4230-a227-0e30c6be189c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# mobile_net\n",
        "model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
        "top_model = Sequential()\n",
        "top_model.add(Flatten(input_shape=model.output_shape[1:])) # 1番目は個数、その後の数を使う\n",
        "top_model.add(Dense(256, activation='relu'))\n",
        "top_model.add(Dropout(0.5))\n",
        "top_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model = Model(inputs=model.input, outputs=top_model(model.output))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWgCsvWpPl3k"
      },
      "source": [
        "# レイアを調べる\n",
        "for i, layer in enumerate(model.layers):\n",
        "    print(i, layer.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLA4PtdyPhCt",
        "outputId": "90a78bde-1b22-42f0-e292-7f50ab333bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 最後のブロックだけ学習させる\n",
        "# for layer in model.layers[0:154]:\n",
        "#    layer.trainable = False\n",
        "\n",
        "# block_16学習\n",
        "for layer in model.layers[0:143]:\n",
        "    layer.trainable = False\n",
        "opt = Adam(lr=0.0001)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "                    steps_per_epoch=len(X_train) / 32, epochs=150)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, batch_size=32)\n",
        "print(score)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "398/397 [==============================] - 69s 172ms/step - loss: 3.1859 - accuracy: 0.2167\n",
            "Epoch 2/150\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 2.2796 - accuracy: 0.4207\n",
            "Epoch 3/150\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 1.9496 - accuracy: 0.4992\n",
            "Epoch 4/150\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 1.6981 - accuracy: 0.5570\n",
            "Epoch 5/150\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 1.5346 - accuracy: 0.5965\n",
            "Epoch 6/150\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 1.4295 - accuracy: 0.6220\n",
            "Epoch 7/150\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 1.3337 - accuracy: 0.6462\n",
            "Epoch 8/150\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 1.2374 - accuracy: 0.6701\n",
            "Epoch 9/150\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 1.1439 - accuracy: 0.6915\n",
            "Epoch 10/150\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 1.1011 - accuracy: 0.6995\n",
            "Epoch 11/150\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 1.0369 - accuracy: 0.7154\n",
            "Epoch 12/150\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 0.9846 - accuracy: 0.7279\n",
            "Epoch 13/150\n",
            "398/397 [==============================] - 69s 172ms/step - loss: 0.9040 - accuracy: 0.7485\n",
            "Epoch 14/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.8857 - accuracy: 0.7506\n",
            "Epoch 15/150\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 0.8609 - accuracy: 0.7578\n",
            "Epoch 16/150\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 0.7867 - accuracy: 0.7744\n",
            "Epoch 17/150\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 0.7552 - accuracy: 0.7830\n",
            "Epoch 18/150\n",
            "398/397 [==============================] - 65s 164ms/step - loss: 0.7475 - accuracy: 0.7889\n",
            "Epoch 19/150\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 0.7138 - accuracy: 0.7960\n",
            "Epoch 20/150\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 0.6739 - accuracy: 0.8052\n",
            "Epoch 21/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.6437 - accuracy: 0.8134\n",
            "Epoch 22/150\n",
            "398/397 [==============================] - 69s 172ms/step - loss: 0.6165 - accuracy: 0.8204\n",
            "Epoch 23/150\n",
            "398/397 [==============================] - 69s 172ms/step - loss: 0.5961 - accuracy: 0.8266\n",
            "Epoch 24/150\n",
            "398/397 [==============================] - 68s 172ms/step - loss: 0.5800 - accuracy: 0.8298\n",
            "Epoch 25/150\n",
            "398/397 [==============================] - 68s 172ms/step - loss: 0.5450 - accuracy: 0.8397\n",
            "Epoch 26/150\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 0.5683 - accuracy: 0.8337\n",
            "Epoch 27/150\n",
            "398/397 [==============================] - 68s 172ms/step - loss: 0.5321 - accuracy: 0.8436\n",
            "Epoch 28/150\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 0.5126 - accuracy: 0.8485\n",
            "Epoch 29/150\n",
            "398/397 [==============================] - 69s 172ms/step - loss: 0.4962 - accuracy: 0.8549\n",
            "Epoch 30/150\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 0.4588 - accuracy: 0.8621\n",
            "Epoch 31/150\n",
            "398/397 [==============================] - 66s 167ms/step - loss: 0.4645 - accuracy: 0.8616\n",
            "Epoch 32/150\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 0.4392 - accuracy: 0.8708\n",
            "Epoch 33/150\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 0.4345 - accuracy: 0.8675\n",
            "Epoch 34/150\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 0.4268 - accuracy: 0.8719\n",
            "Epoch 35/150\n",
            "398/397 [==============================] - 65s 164ms/step - loss: 0.3995 - accuracy: 0.8808\n",
            "Epoch 36/150\n",
            "398/397 [==============================] - 67s 167ms/step - loss: 0.3870 - accuracy: 0.8811\n",
            "Epoch 37/150\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 0.3724 - accuracy: 0.8898\n",
            "Epoch 38/150\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 0.3923 - accuracy: 0.8839\n",
            "Epoch 39/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.3591 - accuracy: 0.8884\n",
            "Epoch 40/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.3675 - accuracy: 0.8903\n",
            "Epoch 41/150\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 0.3429 - accuracy: 0.8935\n",
            "Epoch 42/150\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 0.3242 - accuracy: 0.9020\n",
            "Epoch 43/150\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 0.3326 - accuracy: 0.9017\n",
            "Epoch 44/150\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 0.3395 - accuracy: 0.8981\n",
            "Epoch 45/150\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 0.3275 - accuracy: 0.9045\n",
            "Epoch 46/150\n",
            "398/397 [==============================] - 69s 173ms/step - loss: 0.2993 - accuracy: 0.9104\n",
            "Epoch 47/150\n",
            "398/397 [==============================] - 68s 172ms/step - loss: 0.3140 - accuracy: 0.9054\n",
            "Epoch 48/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.2998 - accuracy: 0.9088\n",
            "Epoch 49/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.2753 - accuracy: 0.9205\n",
            "Epoch 50/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.2836 - accuracy: 0.9169\n",
            "Epoch 51/150\n",
            "398/397 [==============================] - 68s 172ms/step - loss: 0.2762 - accuracy: 0.9161\n",
            "Epoch 52/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.2702 - accuracy: 0.9189\n",
            "Epoch 53/150\n",
            "398/397 [==============================] - 68s 172ms/step - loss: 0.2698 - accuracy: 0.9189\n",
            "Epoch 54/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.2750 - accuracy: 0.9168\n",
            "Epoch 55/150\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 0.2608 - accuracy: 0.9228\n",
            "Epoch 56/150\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 0.2624 - accuracy: 0.9211\n",
            "Epoch 57/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.2372 - accuracy: 0.9285\n",
            "Epoch 58/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.2347 - accuracy: 0.9293\n",
            "Epoch 59/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.2447 - accuracy: 0.9265\n",
            "Epoch 60/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.2428 - accuracy: 0.9302\n",
            "Epoch 61/150\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 0.2457 - accuracy: 0.9238\n",
            "Epoch 62/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.2157 - accuracy: 0.9325\n",
            "Epoch 63/150\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 0.2418 - accuracy: 0.9293\n",
            "Epoch 64/150\n",
            "398/397 [==============================] - 68s 171ms/step - loss: 0.2141 - accuracy: 0.9357\n",
            "Epoch 65/150\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 0.2220 - accuracy: 0.9309\n",
            "Epoch 66/150\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 0.2111 - accuracy: 0.9358\n",
            "Epoch 67/150\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 0.2147 - accuracy: 0.9370\n",
            "Epoch 68/150\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 0.2185 - accuracy: 0.9349\n",
            "Epoch 69/150\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 0.2127 - accuracy: 0.9355\n",
            "Epoch 70/150\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 0.1948 - accuracy: 0.9413\n",
            "Epoch 71/150\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 0.2032 - accuracy: 0.9385\n",
            "Epoch 72/150\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 0.1988 - accuracy: 0.9378\n",
            "Epoch 73/150\n",
            "398/397 [==============================] - 68s 170ms/step - loss: 0.2001 - accuracy: 0.9389\n",
            "Epoch 74/150\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 0.1922 - accuracy: 0.9422\n",
            "Epoch 75/150\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 0.1830 - accuracy: 0.9453\n",
            "Epoch 76/150\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 0.1972 - accuracy: 0.9375\n",
            "Epoch 77/150\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 0.1862 - accuracy: 0.9454\n",
            "Epoch 78/150\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 0.1908 - accuracy: 0.9423\n",
            "Epoch 79/150\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 0.1758 - accuracy: 0.9477\n",
            "Epoch 80/150\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 0.1837 - accuracy: 0.9455\n",
            "Epoch 81/150\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 0.1714 - accuracy: 0.9470\n",
            "Epoch 82/150\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 0.1743 - accuracy: 0.9453\n",
            "Epoch 83/150\n",
            "398/397 [==============================] - 67s 167ms/step - loss: 0.1780 - accuracy: 0.9486\n",
            "Epoch 84/150\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 0.1788 - accuracy: 0.9492\n",
            "Epoch 85/150\n",
            "398/397 [==============================] - 67s 169ms/step - loss: 0.1636 - accuracy: 0.9522\n",
            "Epoch 86/150\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 0.1739 - accuracy: 0.9486\n",
            "Epoch 87/150\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 0.1665 - accuracy: 0.9499\n",
            "Epoch 88/150\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 0.1672 - accuracy: 0.9508\n",
            "Epoch 89/150\n",
            "398/397 [==============================] - 66s 166ms/step - loss: 0.1558 - accuracy: 0.9525\n",
            "Epoch 90/150\n",
            "398/397 [==============================] - 66s 165ms/step - loss: 0.1628 - accuracy: 0.9503\n",
            "Epoch 91/150\n",
            "398/397 [==============================] - 66s 165ms/step - loss: 0.1542 - accuracy: 0.9533\n",
            "Epoch 92/150\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 0.1727 - accuracy: 0.9508\n",
            "Epoch 93/150\n",
            "398/397 [==============================] - 67s 167ms/step - loss: 0.1518 - accuracy: 0.9529\n",
            "Epoch 94/150\n",
            "398/397 [==============================] - 67s 168ms/step - loss: 0.1476 - accuracy: 0.9538\n",
            "Epoch 95/150\n",
            "398/397 [==============================] - 66s 165ms/step - loss: 0.1395 - accuracy: 0.9570\n",
            "Epoch 96/150\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 0.1539 - accuracy: 0.9523\n",
            "Epoch 97/150\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 0.1627 - accuracy: 0.9531\n",
            "Epoch 98/150\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 0.1606 - accuracy: 0.9521\n",
            "Epoch 99/150\n",
            "398/397 [==============================] - 65s 163ms/step - loss: 0.1370 - accuracy: 0.9588\n",
            "Epoch 100/150\n",
            "398/397 [==============================] - 64s 162ms/step - loss: 0.1410 - accuracy: 0.9559\n",
            "Epoch 101/150\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 0.1477 - accuracy: 0.9566\n",
            "Epoch 102/150\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 0.1314 - accuracy: 0.9592\n",
            "Epoch 103/150\n",
            "398/397 [==============================] - 63s 160ms/step - loss: 0.1362 - accuracy: 0.9595\n",
            "Epoch 104/150\n",
            "398/397 [==============================] - 64s 162ms/step - loss: 0.1416 - accuracy: 0.9570\n",
            "Epoch 105/150\n",
            "398/397 [==============================] - 64s 161ms/step - loss: 0.1384 - accuracy: 0.9588\n",
            "Epoch 106/150\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 0.1390 - accuracy: 0.9604\n",
            "Epoch 107/150\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 0.1400 - accuracy: 0.9578\n",
            "Epoch 108/150\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 0.1346 - accuracy: 0.9592\n",
            "Epoch 109/150\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 0.1297 - accuracy: 0.9629\n",
            "Epoch 110/150\n",
            "398/397 [==============================] - 64s 162ms/step - loss: 0.1290 - accuracy: 0.9612\n",
            "Epoch 111/150\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 0.1334 - accuracy: 0.9585\n",
            "Epoch 112/150\n",
            "398/397 [==============================] - 64s 160ms/step - loss: 0.1322 - accuracy: 0.9613\n",
            "Epoch 113/150\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 0.1211 - accuracy: 0.9630\n",
            "Epoch 114/150\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 0.1282 - accuracy: 0.9603\n",
            "Epoch 115/150\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 0.1221 - accuracy: 0.9610\n",
            "Epoch 116/150\n",
            "398/397 [==============================] - 63s 159ms/step - loss: 0.1162 - accuracy: 0.9652\n",
            "Epoch 117/150\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 0.1281 - accuracy: 0.9629\n",
            "Epoch 118/150\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 0.1158 - accuracy: 0.9657\n",
            "Epoch 119/150\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 0.1317 - accuracy: 0.9627\n",
            "Epoch 120/150\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 0.1109 - accuracy: 0.9677\n",
            "Epoch 121/150\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 0.1197 - accuracy: 0.9648\n",
            "Epoch 122/150\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 0.1205 - accuracy: 0.9652\n",
            "Epoch 123/150\n",
            "398/397 [==============================] - 63s 158ms/step - loss: 0.1219 - accuracy: 0.9632\n",
            "Epoch 124/150\n",
            "398/397 [==============================] - 61s 154ms/step - loss: 0.1191 - accuracy: 0.9645\n",
            "Epoch 125/150\n",
            "398/397 [==============================] - 62s 155ms/step - loss: 0.1197 - accuracy: 0.9639\n",
            "Epoch 126/150\n",
            "398/397 [==============================] - 63s 157ms/step - loss: 0.1111 - accuracy: 0.9647\n",
            "Epoch 127/150\n",
            "398/397 [==============================] - 62s 156ms/step - loss: 0.1072 - accuracy: 0.9669\n",
            "Epoch 128/150\n",
            "398/397 [==============================] - 62s 156ms/step - loss: 0.1171 - accuracy: 0.9654\n",
            "Epoch 129/150\n",
            "398/397 [==============================] - 62s 155ms/step - loss: 0.1038 - accuracy: 0.9677\n",
            "Epoch 130/150\n",
            "398/397 [==============================] - 62s 155ms/step - loss: 0.0963 - accuracy: 0.9711\n",
            "Epoch 131/150\n",
            "398/397 [==============================] - 61s 154ms/step - loss: 0.1079 - accuracy: 0.9670\n",
            "Epoch 132/150\n",
            "398/397 [==============================] - 61s 153ms/step - loss: 0.1017 - accuracy: 0.9699\n",
            "Epoch 133/150\n",
            "398/397 [==============================] - 61s 153ms/step - loss: 0.1056 - accuracy: 0.9690\n",
            "Epoch 134/150\n",
            "398/397 [==============================] - 61s 154ms/step - loss: 0.0966 - accuracy: 0.9697\n",
            "Epoch 135/150\n",
            "398/397 [==============================] - 61s 154ms/step - loss: 0.1038 - accuracy: 0.9689\n",
            "Epoch 136/150\n",
            "398/397 [==============================] - 61s 154ms/step - loss: 0.1066 - accuracy: 0.9679\n",
            "Epoch 137/150\n",
            "398/397 [==============================] - 61s 154ms/step - loss: 0.1000 - accuracy: 0.9685\n",
            "Epoch 138/150\n",
            "398/397 [==============================] - 61s 154ms/step - loss: 0.0970 - accuracy: 0.9716\n",
            "Epoch 139/150\n",
            "398/397 [==============================] - 62s 155ms/step - loss: 0.1047 - accuracy: 0.9701\n",
            "Epoch 140/150\n",
            "398/397 [==============================] - 62s 156ms/step - loss: 0.1062 - accuracy: 0.9684\n",
            "Epoch 141/150\n",
            "398/397 [==============================] - 63s 157ms/step - loss: 0.1025 - accuracy: 0.9698\n",
            "Epoch 142/150\n",
            "398/397 [==============================] - 62s 155ms/step - loss: 0.0913 - accuracy: 0.9728\n",
            "Epoch 143/150\n",
            "398/397 [==============================] - 61s 153ms/step - loss: 0.0931 - accuracy: 0.9725\n",
            "Epoch 144/150\n",
            "398/397 [==============================] - 61s 154ms/step - loss: 0.0986 - accuracy: 0.9697\n",
            "Epoch 145/150\n",
            "398/397 [==============================] - 61s 154ms/step - loss: 0.0998 - accuracy: 0.9698\n",
            "Epoch 146/150\n",
            "398/397 [==============================] - 61s 154ms/step - loss: 0.1040 - accuracy: 0.9691\n",
            "Epoch 147/150\n",
            "398/397 [==============================] - 61s 154ms/step - loss: 0.0938 - accuracy: 0.9724\n",
            "Epoch 148/150\n",
            "398/397 [==============================] - 61s 153ms/step - loss: 0.0869 - accuracy: 0.9747\n",
            "Epoch 149/150\n",
            "398/397 [==============================] - 61s 153ms/step - loss: 0.0864 - accuracy: 0.9730\n",
            "Epoch 150/150\n",
            "398/397 [==============================] - 61s 154ms/step - loss: 0.0896 - accuracy: 0.9729\n",
            "132/132 [==============================] - 3s 24ms/step - loss: 1.7078 - accuracy: 0.7812\n",
            "[1.7078317403793335, 0.7812351584434509]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_ca7oh6gduB"
      },
      "source": [
        "model.save('./food_recognition_mobile.h5')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvT54if1gmTo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def process(data):\n",
        "    plt.figure()\n",
        "    plt.imshow(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2zr9jQORdML",
        "outputId": "cdb0cd9d-d607-4a05-8302-01c11428b35f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4210, 150, 150, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7l7T7oP3N5a"
      },
      "source": [
        "# 比較用 800*800 トリミング\n",
        "for index, classlabel in enumerate(classes):\n",
        "    photos_dir = './food/' + classlabel  \n",
        "    files = glob.glob(photos_dir + '/*.jpg')\n",
        "    for i, file in enumerate(files):\n",
        "        image = Image.open(file)\n",
        "        image = image.convert('RGB')\n",
        "        # image = image.convert('RGB')の前にトリミングするとメモリオーバー、理由不明\n",
        "        image = crop_center(image, 800, 800) # resizeする時にアスペクト比を固定させるため、800*800の部分をトリミング\n",
        "        image = image.resize((image_size, image_size))\n",
        "        data = np.asarray(image, dtype=np.float32) / 255.0  # 正規化 # dtype=np.float32でサイズ縮小、MemoryError解消のため # 2020/7/7\n",
        "        data = np.array(data)\n",
        "        process(data)\n",
        "        X = []\n",
        "        # 最後尾に追加する\n",
        "        X.append(data)\n",
        "        X = np.array(X)\n",
        "        result = model.predict([X])[0]\n",
        "        # 値の大きい方の番号(配列の添字)を返す\n",
        "        predicted = result.argmax()\n",
        "        percentage = int(result[predicted] * 100)\n",
        "        print(classes[predicted], percentage)\n",
        "        print()\n",
        "        if i > 0:\n",
        "          break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcEQS1dd58Wf"
      },
      "source": [
        "for index, classlabel in enumerate(classes):\n",
        "    photos_dir = './food/' + classlabel  \n",
        "    files = glob.glob(photos_dir + '/*.jpg')\n",
        "    for i, file in enumerate(files):\n",
        "        image = Image.open(file)\n",
        "        image = image.convert('RGB')\n",
        "        w, h = image.size\n",
        "        image = crop_center(image, w, w) if w >= h else crop_center(image, h, h)\n",
        "        image = image.resize((image_size, image_size))\n",
        "        data = np.asarray(image, dtype=np.float32) / 255.0  # 正規化 # dtype=np.float32でサイズ縮小、MemoryError解消のため # 2020/7/7\n",
        "        data = np.array(data)\n",
        "        process(data)\n",
        "        X = []\n",
        "        # 最後尾に追加する\n",
        "        X.append(data)\n",
        "        X = np.array(X)\n",
        "        result = model.predict([X])[0]\n",
        "        # 値の大きい方の番号(配列の添字)を返す\n",
        "        predicted = result.argmax()\n",
        "        percentage = int(result[predicted] * 100)\n",
        "        print(classes[predicted], percentage)\n",
        "        print()\n",
        "        if i > 0:\n",
        "          break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brnVukT0pf5J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}